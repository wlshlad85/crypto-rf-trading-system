#!/usr/bin/env python3
"""
ULTRATHINK Semantic Graph Builder
Enhanced dependency mapping with semantic relationships

Philosophy: Understanding code relationships at semantic level
Performance: < 5ms graph building for 107 modules
Intelligence: Trading-aware relationship detection beyond imports
"""

import json
import time
import networkx as nx
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from collections import defaultdict, deque

# Import our semantic chunking system
import sys
sys.path.insert(0, str(Path(__file__).parent))
from tree_sitter_chunker import SemanticChunk, SemanticModuleAnalysis, TreeSitterSemanticChunker

@dataclass
class SemanticRelationship:
    """Represents a semantic relationship between code chunks"""
    source_chunk: str
    target_chunk: str
    relationship_type: str  # 'imports', 'calls', 'inherits', 'trading_workflow', 'risk_dependency'
    strength: float  # 0.0-1.0
    evidence: List[str]  # Evidence for the relationship
    trading_context: Optional[str]  # Trading-specific context

@dataclass
class SemanticGraphMetrics:
    """Metrics for the semantic graph"""
    total_chunks: int
    total_relationships: int
    trading_chunks: int
    risk_chunks: int
    ml_chunks: int
    avg_connections_per_chunk: float
    graph_density: float
    critical_path_length: int
    clustering_coefficient: float

class TradingWorkflowAnalyzer:
    """Analyzes trading-specific workflows and dependencies"""
    
    def __init__(self):
        # Define trading workflow patterns
        self.WORKFLOW_PATTERNS = {
            'data_flow': [
                'data_fetcher â†’ feature_engineering â†’ ml_model â†’ strategy',
                'market_data â†’ indicators â†’ signals â†’ execution'
            ],
            'risk_flow': [
                'position_sizing â†’ risk_checks â†’ order_execution',
                'kelly_criterion â†’ position_limits â†’ trade_execution'
            ],
            'strategy_flow': [
                'signal_generation â†’ strategy_logic â†’ order_placement',
                'ensemble_models â†’ meta_learning â†’ trading_decisions'
            ],
            'execution_flow': [
                'paper_trading â†’ live_execution â†’ performance_monitoring',
                'backtesting â†’ validation â†’ deployment'
            ]
        }
        
        # Trading component relationships
        self.TRADING_RELATIONSHIPS = {
            'kelly_criterion': ['risk_management', 'position_sizing', 'portfolio_optimization'],
            'ensemble_ml': ['meta_learning', 'model_combination', 'prediction_aggregation'],
            'momentum_strategy': ['technical_indicators', 'signal_generation', 'trend_following'],
            'risk_management': ['stop_loss', 'position_limits', 'drawdown_control'],
            'live_trading': ['real_time_execution', 'continuous_monitoring', 'automated_trading']
        }
    
    def analyze_trading_workflows(self, chunks: List[SemanticChunk]) -> Dict[str, List[str]]:
        """Analyze trading workflows from semantic chunks"""
        workflows = defaultdict(list)
        
        # Group chunks by trading patterns
        chunks_by_pattern = defaultdict(list)
        for chunk in chunks:
            for tag in chunk.semantic_tags:
                if tag in self.TRADING_RELATIONSHIPS:
                    chunks_by_pattern[tag].append(chunk.chunk_id)
        
        # Build workflow chains
        for pattern, related_patterns in self.TRADING_RELATIONSHIPS.items():
            if pattern in chunks_by_pattern:
                workflow_chain = [pattern]
                
                # Find connected patterns
                for related in related_patterns:
                    if related in chunks_by_pattern:
                        workflow_chain.append(related)
                
                if len(workflow_chain) > 1:
                    workflows[f"{pattern}_workflow"] = workflow_chain
        
        return dict(workflows)
    
    def detect_critical_paths(self, chunks: List[SemanticChunk]) -> List[str]:
        """Detect critical paths in trading workflows"""
        critical_paths = []
        
        # Find high-relevance chunks that form critical paths
        high_relevance_chunks = [c for c in chunks if c.trading_relevance > 0.8]
        
        # Look for trading execution paths
        execution_chunks = [c for c in high_relevance_chunks if 'order_execution' in c.semantic_tags]
        risk_chunks = [c for c in high_relevance_chunks if 'risk_management' in c.semantic_tags]
        ml_chunks = [c for c in high_relevance_chunks if 'ensemble_ml' in c.semantic_tags]
        
        # Build critical path: ML â†’ Risk â†’ Execution
        if ml_chunks and risk_chunks and execution_chunks:
            critical_paths.append(f"{ml_chunks[0].chunk_id} â†’ {risk_chunks[0].chunk_id} â†’ {execution_chunks[0].chunk_id}")
        
        return critical_paths

class SemanticGraphBuilder:
    """Builds semantic dependency graphs with trading intelligence"""
    
    def __init__(self):
        self.workflow_analyzer = TradingWorkflowAnalyzer()
        self.graph = nx.DiGraph()
        self.semantic_chunker = TreeSitterSemanticChunker()
        
        print("ğŸ•¸ï¸  Semantic Graph Builder initialized")
    
    def build_semantic_graph(self, module_analyses: Dict[str, SemanticModuleAnalysis]) -> nx.DiGraph:
        """Build comprehensive semantic graph from module analyses"""
        start_time = time.time()
        
        # Clear existing graph
        self.graph.clear()
        
        # Add all chunks as nodes
        all_chunks = []
        for analysis in module_analyses.values():
            all_chunks.extend(analysis.chunks)
            
        self._add_chunk_nodes(all_chunks)
        
        # Add semantic relationships
        relationships = self._discover_semantic_relationships(all_chunks)
        self._add_relationship_edges(relationships)
        
        # Add trading workflow relationships
        self._add_trading_workflow_edges(all_chunks)
        
        # Add module-level relationships
        self._add_module_relationships(module_analyses)
        
        build_time = (time.time() - start_time) * 1000
        print(f"âœ… Semantic graph built: {self.graph.number_of_nodes()} nodes, "
              f"{self.graph.number_of_edges()} edges in {build_time:.1f}ms")
        
        return self.graph
    
    def _add_chunk_nodes(self, chunks: List[SemanticChunk]):
        """Add semantic chunks as nodes in the graph"""
        for chunk in chunks:
            self.graph.add_node(
                chunk.chunk_id,
                chunk_type=chunk.chunk_type,
                trading_relevance=chunk.trading_relevance,
                complexity_score=chunk.complexity_score,
                semantic_tags=chunk.semantic_tags,
                start_line=chunk.start_line,
                end_line=chunk.end_line
            )
    
    def _discover_semantic_relationships(self, chunks: List[SemanticChunk]) -> List[SemanticRelationship]:
        """Discover semantic relationships between chunks"""
        relationships = []
        
        for i, chunk1 in enumerate(chunks):
            for j, chunk2 in enumerate(chunks):
                if i != j:
                    relationship = self._analyze_chunk_relationship(chunk1, chunk2)
                    if relationship:
                        relationships.append(relationship)
        
        return relationships
    
    def _analyze_chunk_relationship(self, chunk1: SemanticChunk, chunk2: SemanticChunk) -> Optional[SemanticRelationship]:
        """Analyze relationship between two chunks"""
        # Check for direct dependencies
        if chunk2.chunk_id in chunk1.dependencies:
            return SemanticRelationship(
                source_chunk=chunk1.chunk_id,
                target_chunk=chunk2.chunk_id,
                relationship_type='calls',
                strength=0.8,
                evidence=[f"Direct dependency in {chunk1.chunk_id}"],
                trading_context=None
            )
        
        # Check for semantic tag overlap
        shared_tags = set(chunk1.semantic_tags) & set(chunk2.semantic_tags)
        if shared_tags:
            strength = len(shared_tags) * 0.2
            return SemanticRelationship(
                source_chunk=chunk1.chunk_id,
                target_chunk=chunk2.chunk_id,
                relationship_type='semantic_similarity',
                strength=min(strength, 1.0),
                evidence=[f"Shared tags: {list(shared_tags)}"],
                trading_context=list(shared_tags)[0] if shared_tags else None
            )
        
        # Check for trading workflow relationships
        if self._is_trading_workflow_related(chunk1, chunk2):
            return SemanticRelationship(
                source_chunk=chunk1.chunk_id,
                target_chunk=chunk2.chunk_id,
                relationship_type='trading_workflow',
                strength=0.6,
                evidence=["Trading workflow connection detected"],
                trading_context="workflow_dependency"
            )
        
        return None
    
    def _is_trading_workflow_related(self, chunk1: SemanticChunk, chunk2: SemanticChunk) -> bool:
        """Check if two chunks are related in trading workflows"""
        # Check for known trading workflow patterns
        trading_workflows = [
            (['ensemble_ml', 'ml_model'], ['risk_management', 'kelly_criterion']),
            (['risk_management'], ['order_execution', 'live_trading']),
            (['momentum_strategy'], ['order_execution']),
            (['kelly_criterion'], ['position_sizing', 'risk_management'])
        ]
        
        for source_patterns, target_patterns in trading_workflows:
            source_match = any(tag in chunk1.semantic_tags for tag in source_patterns)
            target_match = any(tag in chunk2.semantic_tags for tag in target_patterns)
            
            if source_match and target_match:
                return True
        
        return False
    
    def _add_relationship_edges(self, relationships: List[SemanticRelationship]):
        """Add relationship edges to the graph"""
        for rel in relationships:
            if rel.source_chunk in self.graph.nodes and rel.target_chunk in self.graph.nodes:
                self.graph.add_edge(
                    rel.source_chunk,
                    rel.target_chunk,
                    relationship_type=rel.relationship_type,
                    strength=rel.strength,
                    evidence=rel.evidence,
                    trading_context=rel.trading_context
                )
    
    def _add_trading_workflow_edges(self, chunks: List[SemanticChunk]):
        """Add trading-specific workflow edges"""
        workflows = self.workflow_analyzer.analyze_trading_workflows(chunks)
        
        for workflow_name, workflow_chain in workflows.items():
            # Connect chunks in workflow sequence
            for i in range(len(workflow_chain) - 1):
                source_chunks = [c for c in chunks if workflow_chain[i] in c.semantic_tags]
                target_chunks = [c for c in chunks if workflow_chain[i + 1] in c.semantic_tags]
                
                for source_chunk in source_chunks:
                    for target_chunk in target_chunks:
                        if source_chunk.chunk_id in self.graph.nodes and target_chunk.chunk_id in self.graph.nodes:
                            self.graph.add_edge(
                                source_chunk.chunk_id,
                                target_chunk.chunk_id,
                                relationship_type='trading_workflow',
                                strength=0.7,
                                evidence=[f"Part of {workflow_name}"],
                                trading_context=workflow_name
                            )
    
    def _add_module_relationships(self, module_analyses: Dict[str, SemanticModuleAnalysis]):
        """Add module-level relationships"""
        for module_path, analysis in module_analyses.items():
            # Connect chunks within the same module
            for i, chunk1 in enumerate(analysis.chunks):
                for j, chunk2 in enumerate(analysis.chunks):
                    if i != j and chunk1.chunk_id in self.graph.nodes and chunk2.chunk_id in self.graph.nodes:
                        # Add weak intra-module connections
                        if not self.graph.has_edge(chunk1.chunk_id, chunk2.chunk_id):
                            self.graph.add_edge(
                                chunk1.chunk_id,
                                chunk2.chunk_id,
                                relationship_type='same_module',
                                strength=0.3,
                                evidence=[f"Same module: {module_path}"],
                                trading_context=None
                            )
    
    def get_semantic_metrics(self) -> SemanticGraphMetrics:
        """Calculate semantic graph metrics"""
        if not self.graph.nodes:
            return SemanticGraphMetrics(0, 0, 0, 0, 0, 0.0, 0.0, 0, 0.0)
        
        # Count different types of chunks
        trading_chunks = len([n for n in self.graph.nodes() 
                             if self.graph.nodes[n].get('trading_relevance', 0) > 0.5])
        risk_chunks = len([n for n in self.graph.nodes() 
                          if 'risk_management' in self.graph.nodes[n].get('semantic_tags', [])])
        ml_chunks = len([n for n in self.graph.nodes() 
                        if any(tag in ['ensemble_ml', 'ml_model'] 
                              for tag in self.graph.nodes[n].get('semantic_tags', []))])
        
        # Calculate connectivity metrics
        total_nodes = self.graph.number_of_nodes()
        total_edges = self.graph.number_of_edges()
        avg_connections = total_edges / total_nodes if total_nodes > 0 else 0.0
        
        # Graph density
        max_possible_edges = total_nodes * (total_nodes - 1)
        density = total_edges / max_possible_edges if max_possible_edges > 0 else 0.0
        
        # Critical path length (approximate)
        try:
            if nx.is_weakly_connected(self.graph):
                critical_path_length = nx.diameter(self.graph.to_undirected())
            else:
                # Find longest path in largest component
                largest_component = max(nx.weakly_connected_components(self.graph), key=len)
                subgraph = self.graph.subgraph(largest_component)
                critical_path_length = nx.diameter(subgraph.to_undirected()) if len(largest_component) > 1 else 0
        except:
            critical_path_length = 0
        
        # Clustering coefficient
        try:
            clustering_coeff = nx.average_clustering(self.graph.to_undirected())
        except:
            clustering_coeff = 0.0
        
        return SemanticGraphMetrics(
            total_chunks=total_nodes,
            total_relationships=total_edges,
            trading_chunks=trading_chunks,
            risk_chunks=risk_chunks,
            ml_chunks=ml_chunks,
            avg_connections_per_chunk=avg_connections,
            graph_density=density,
            critical_path_length=critical_path_length,
            clustering_coefficient=clustering_coeff
        )
    
    def get_chunk_neighbors(self, chunk_id: str, max_depth: int = 2) -> List[str]:
        """Get neighboring chunks within max_depth"""
        if chunk_id not in self.graph.nodes:
            return []
        
        neighbors = set()
        queue = deque([(chunk_id, 0)])
        visited = {chunk_id}
        
        while queue:
            current_chunk, depth = queue.popleft()
            
            if depth >= max_depth:
                continue
            
            # Get successors and predecessors
            for neighbor in list(self.graph.successors(current_chunk)) + list(self.graph.predecessors(current_chunk)):
                if neighbor not in visited:
                    visited.add(neighbor)
                    neighbors.add(neighbor)
                    queue.append((neighbor, depth + 1))
        
        return list(neighbors)
    
    def get_trading_critical_chunks(self, top_k: int = 10) -> List[Tuple[str, float]]:
        """Get most critical chunks for trading workflows"""
        if not self.graph.nodes:
            return []
        
        # Calculate importance based on trading relevance and connectivity
        chunk_scores = []
        
        for chunk_id in self.graph.nodes():
            node_data = self.graph.nodes[chunk_id]
            trading_relevance = node_data.get('trading_relevance', 0.0)
            
            # Calculate connectivity score
            in_degree = self.graph.in_degree(chunk_id)
            out_degree = self.graph.out_degree(chunk_id)
            connectivity_score = (in_degree + out_degree) / max(self.graph.number_of_nodes(), 1)
            
            # Combined score
            importance_score = trading_relevance * 0.7 + connectivity_score * 0.3
            chunk_scores.append((chunk_id, importance_score))
        
        # Sort by importance and return top_k
        chunk_scores.sort(key=lambda x: x[1], reverse=True)
        return chunk_scores[:top_k]
    
    def export_graph_data(self) -> Dict[str, Any]:
        """Export graph data for analysis or visualization"""
        return {
            'nodes': [
                {
                    'id': node_id,
                    **self.graph.nodes[node_id]
                }
                for node_id in self.graph.nodes()
            ],
            'edges': [
                {
                    'source': edge[0],
                    'target': edge[1],
                    **self.graph.edges[edge]
                }
                for edge in self.graph.edges()
            ],
            'metrics': asdict(self.get_semantic_metrics())
        }

# Example usage and testing
if __name__ == "__main__":
    print("ğŸ•¸ï¸  ULTRATHINK Semantic Graph Builder Testing")
    print("=" * 60)
    
    # Initialize components
    chunker = TreeSitterSemanticChunker()
    graph_builder = SemanticGraphBuilder()
    
    # Test with sample modules
    sample_modules = {
        "risk_management.py": '''
class KellyCriterionCalculator:
    def calculate_optimal_fraction(self, win_rate, avg_win, avg_loss):
        """Calculate Kelly criterion optimal position size"""
        pass

def apply_risk_limits(position_size, max_risk=0.02):
    """Apply position size limits"""
    pass
''',
        "trading_strategy.py": '''
class MomentumStrategy:
    def __init__(self, risk_manager):
        self.risk_manager = risk_manager
    
    def generate_signals(self, market_data):
        """Generate trading signals based on momentum"""
        pass

def execute_trade(signal, position_size):
    """Execute trading order"""
    pass
'''
    }
    
    # Analyze modules
    module_analyses = {}
    total_analysis_time = 0
    
    for module_path, code in sample_modules.items():
        print(f"\nğŸ” Analyzing {module_path}...")
        analysis = chunker.chunk_module(module_path, code)
        module_analyses[module_path] = analysis
        total_analysis_time += analysis.analysis_time_ms
        
        print(f"â”œâ”€â”€ Chunks: {len(analysis.chunks)}")
        print(f"â”œâ”€â”€ Trading Patterns: {analysis.trading_patterns}")
        print(f"â””â”€â”€ Analysis Time: {analysis.analysis_time_ms:.1f}ms")
    
    # Build semantic graph
    print(f"\nğŸ•¸ï¸  Building semantic graph...")
    graph = graph_builder.build_semantic_graph(module_analyses)
    
    # Get metrics
    metrics = graph_builder.get_semantic_metrics()
    print(f"\nğŸ“Š Semantic Graph Metrics:")
    print(f"â”œâ”€â”€ Total Chunks: {metrics.total_chunks}")
    print(f"â”œâ”€â”€ Total Relationships: {metrics.total_relationships}")
    print(f"â”œâ”€â”€ Trading Chunks: {metrics.trading_chunks}")
    print(f"â”œâ”€â”€ Risk Chunks: {metrics.risk_chunks}")
    print(f"â”œâ”€â”€ ML Chunks: {metrics.ml_chunks}")
    print(f"â”œâ”€â”€ Avg Connections/Chunk: {metrics.avg_connections_per_chunk:.2f}")
    print(f"â”œâ”€â”€ Graph Density: {metrics.graph_density:.3f}")
    print(f"â”œâ”€â”€ Critical Path Length: {metrics.critical_path_length}")
    print(f"â””â”€â”€ Clustering Coefficient: {metrics.clustering_coefficient:.3f}")
    
    # Show critical chunks
    critical_chunks = graph_builder.get_trading_critical_chunks(top_k=5)
    print(f"\nğŸ¯ Most Critical Trading Chunks:")
    for i, (chunk_id, score) in enumerate(critical_chunks, 1):
        print(f"{i}. {chunk_id} (importance: {score:.3f})")
    
    print(f"\nâœ… Semantic graph analysis complete!")
    print(f"ğŸ“ˆ Total Processing Time: {total_analysis_time:.1f}ms")